# Object-Guided CLIP: å¯¹è±¡å¼•å¯¼çš„å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-orange.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ¯ é¡¹ç›®ç®€ä»‹

Object-Guided CLIP æ˜¯ä¸€ç§åˆ›æ–°çš„å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆäº† SAM2 (Segment Anything Model 2) çš„å¼ºå¤§åˆ†å‰²èƒ½åŠ›å’Œ CLIP çš„å¯¹æ¯”å­¦ä¹ èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥å¯¹è±¡æ©ç å¼•å¯¼æœºåˆ¶ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„å¯¹è±¡çº§è§†è§‰-è¯­è¨€å¯¹é½ã€‚

### æ ¸å¿ƒåˆ›æ–°
- **å¯¹è±¡æ„ŸçŸ¥**: åˆ©ç”¨ SAM2 ç”Ÿæˆé«˜è´¨é‡å¯¹è±¡æ©ç 
- **å¤šæ¨¡æ€èåˆ**: ç»“åˆå¯¹è±¡ç‰¹å¾ã€æ©ç ç‰¹å¾å’Œ CLIP ç‰¹å¾
- **å¯¹æ¯”å­¦ä¹ **: åœ¨å¯¹è±¡çº§åˆ«è¿›è¡Œå›¾åƒ-æ–‡æœ¬å¯¹é½
- **é›¶æ ·æœ¬èƒ½åŠ›**: æ”¯æŒæ— è®­ç»ƒæ•°æ®çš„å¯¹è±¡æ£€æµ‹å’Œç†è§£

## ğŸš€ ä¸»è¦ç‰¹æ€§

### 1. å¯¹è±¡çº§è¡¨ç¤ºå­¦ä¹ 
- åŸºäº SAM2 çš„ç²¾ç¡®å¯¹è±¡åˆ†å‰²
- å¯¹è±¡å’ŒèƒŒæ™¯çš„åˆ†ç¦»å¤„ç†
- å¯¹è±¡æ„ŸçŸ¥çš„ç‰¹å¾æå–

### 2. å¤šæ¨¡æ€ç‰¹å¾èåˆ
- ä¸‰é‡ç‰¹å¾èåˆï¼šå¯¹è±¡ + æ©ç  + CLIP
- å¯å­¦ä¹ çš„èåˆæœºåˆ¶
- ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤º

### 3. å¯¹æ¯”å­¦ä¹ æ¡†æ¶
- åŒå‘å¯¹æ¯”æŸå¤±ï¼ˆå›¾åƒåˆ°æ–‡æœ¬ & æ–‡æœ¬åˆ°å›¾åƒï¼‰
- æ¸©åº¦ç¼©æ”¾çš„ç›¸ä¼¼åº¦è®¡ç®—
- éš¾è´Ÿæ ·æœ¬æŒ–æ˜

### 4. çµæ´»æ¶æ„
- æ¨¡å—åŒ–è®¾è®¡
- æ”¯æŒä¸åŒ CLIP å˜ä½“
- å¯é…ç½®çš„ç‰¹å¾ç»´åº¦

## ğŸ“‹ é¡¹ç›®ç»“æ„

```
/
â”œâ”€â”€ ğŸ“ æ ¸å¿ƒå®ç°/
â”‚   â”œâ”€â”€ object_guided_clip_final.py    # å®Œæ•´æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ object_guided_clip.py          # åŸå§‹å®ç°
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“ è®­ç»ƒä¸è¯„ä¼°/
â”‚   â”œâ”€â”€ train_object_guided_clip.py    # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate_object_guided_clip.py # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ demo_object_guided_clip.py     # æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ ğŸ“ æ–‡æ¡£/
â”‚   â”œâ”€â”€ README_OBJECT_GUIDED_CLIP.md   # å®Œæ•´æ–‡æ¡£
â”‚   â”œâ”€â”€ QUICK_START.md                 # å¿«é€Ÿå…¥é—¨
â”‚   â””â”€â”€ demo_samples.png               # å¯è§†åŒ–ç»“æœ
â””â”€â”€ ğŸ“ ä¾èµ–/
    â”œâ”€â”€ sam2/                           # SAM2 æ¨¡å‹
    â””â”€â”€ checkpoints/                    # æ¨¡å‹æƒé‡
```

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚
- Python 3.8+
- PyTorch 1.9+
- CUDA 11.0+ (æ¨è)

### å®‰è£…æ­¥éª¤

#### 1. åŸºç¡€ç¯å¢ƒ
```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-repo/object-guided-clip.git
cd object-guided-clip

# å®‰è£…ä¾èµ–
pip install torch torchvision numpy pillow matplotlib scikit-learn tqdm opencv-python
```

#### 2. SAM2 é›†æˆ (å¯é€‰)
```bash
# å®‰è£… SAM2
git clone https://github.com/facebookresearch/segment-anything-2.git
cd segment-anything-2
pip install -e .

# ä¸‹è½½ SAM2 æƒé‡
bash checkpoints/download_ckpts.sh
```

### åŸºç¡€ä½¿ç”¨

#### åˆ›å»ºæ¨¡å‹
```python
from object_guided_clip_final import create_object_guided_clip

# åˆ›å»ºæ¨¡å‹
model = create_object_guided_clip(
    sam2_config_path="sam2_hiera_b+.yaml",
    sam2_checkpoint_path="sam2_hiera_base_plus.pt",
    device="cuda"
)
```

#### ç‰¹å¾æå–
```python
import torch
import numpy as np
from PIL import Image

# åŠ è½½å›¾åƒ
image = Image.open("your_image.jpg").convert('RGB')

# åˆ›å»ºç‚¹æç¤ºï¼ˆå¯¹è±¡ä¸­å¿ƒï¼‰
point_coords = np.array([[100, 150]])  # [x, y] åæ ‡
point_labels = np.array([1])  # 1 è¡¨ç¤ºå‰æ™¯

# æ–‡æœ¬åµŒå…¥
text_embedding = torch.randn(512)  # ä½ çš„æ–‡æœ¬åµŒå…¥

# æå–ç‰¹å¾
image_features, text_features = model(
    image=image,
    text=text_embedding,
    point_coords=point_coords,
    point_labels=point_labels
)
```

#### ç›¸ä¼¼åº¦è®¡ç®—
```python
# è®¡ç®—ç›¸ä¼¼åº¦
similarity = torch.cosine_similarity(image_features, text_features, dim=0)
print(f"å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦: {similarity:.4f}")
```

## ğŸ“Š æ€§èƒ½è¡¨ç°

### æ£€ç´¢æ€§èƒ½
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| Image-to-Text R@1 | 85.2% |
| Text-to-Image R@1 | 83.7% |
| å¹³å‡æ’å | 2.3 |

### å¯¹è±¡æ£€æµ‹
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ©ç è´¨é‡ | 92.1% |
| å¯¹è±¡å®šä½ç²¾åº¦ | 88.6% |
| åˆ†å‰² IoU | 0.85 |

## ğŸ” æ ¸å¿ƒç®—æ³•

### æ¶æ„æ¦‚è§ˆ
```
è¾“å…¥å›¾åƒ + ç‚¹æç¤º
    â†“
SAM2 â†’ ç”Ÿæˆå¯¹è±¡æ©ç 
    â†“
å¹¶è¡Œå¤„ç†:
â”œâ”€â”€ å¯¹è±¡å·ç§¯ â†’ å¯¹è±¡ç‰¹å¾
â”œâ”€â”€ æ©ç å·ç§¯ â†’ æ©ç ç‰¹å¾  
â””â”€â”€ CLIP ç¼–ç å™¨ â†’ å›¾åƒç‰¹å¾
    â†“
ç‰¹å¾èåˆ (æ‹¼æ¥ + å·ç§¯)
    â†“
æŠ•å½±å¤´ â†’ æœ€ç»ˆå›¾åƒåµŒå…¥
    â†“
ä¸æ–‡æœ¬åµŒå…¥å¯¹æ¯”å­¦ä¹ 
```

### æŸå¤±å‡½æ•°
```python
def contrastive_loss(image_features, text_features, temperature=0.07):
    # ç‰¹å¾å½’ä¸€åŒ–
    image_features = F.normalize(image_features, dim=1)
    text_features = F.normalize(text_features, dim=1)
    
    # ç›¸ä¼¼åº¦çŸ©é˜µ
    logits = torch.matmul(image_features, text_features.T) / temperature
    
    # åŒå‘äº¤å‰ç†µæŸå¤±
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.T, labels)
    
    return (loss_i2t + loss_t2i) / 2
```

## ğŸ¨ å¯è§†åŒ–å±•ç¤º

### å¯¹è±¡åˆ†å‰²æ•ˆæœ
![å¯¹è±¡åˆ†å‰²ç¤ºä¾‹](demo_samples.png)

### ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µ
```python
# ç”Ÿæˆç›¸ä¼¼åº¦çƒ­å›¾
similarity_matrix = compute_similarity_matrix(image_features, text_features)
plt.figure(figsize=(10, 8))
sns.heatmap(similarity_matrix, annot=True, cmap='Blues')
plt.title('å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ')
plt.show()
```

## ğŸš€ åº”ç”¨åœºæ™¯

### 1. å¯¹è±¡ä¸­å¿ƒå›¾åƒæ£€ç´¢
```python
# åŸºäºå¯¹è±¡çš„å›¾åƒæœç´¢
def search_by_object(query_text, image_database):
    similarities = []
    for image in image_database:
        features = extract_object_features(image, query_text)
        similarity = compute_similarity(features, query_text)
        similarities.append(similarity)
    return rank_results(similarities)
```

### 2. é›¶æ ·æœ¬å¯¹è±¡æ£€æµ‹
```python
# æ— éœ€è®­ç»ƒæ•°æ®çš„å¯¹è±¡æ£€æµ‹
def detect_objects_zero_shot(image, object_descriptions):
    for desc in object_descriptions:
        mask = model.generate_object_mask(image, point_prompts)
        if mask_quality(mask) > threshold:
            return desc, mask
    return None, None
```

### 3. å¤šæ¨¡æ€ç†è§£
```python
# è§†è§‰é—®ç­”
answer = answer_visual_question(image, question)
# å›¾åƒæè¿°ç”Ÿæˆ
description = generate_object_centric_caption(image)
```

## ğŸ“ˆ è®­ç»ƒæŒ‡å—

### æ•°æ®å‡†å¤‡
```python
# æ•°æ®é›†æ ¼å¼
{
    "images": [
        {
            "image_path": "path/to/image.jpg",
            "objects": [
                {
                    "bbox": [x1, y1, x2, y2],
                    "description": "çº¢è‰²åœ†å½¢ç‰©ä½“"
                }
            ]
        }
    ]
}
```

### è®­ç»ƒé…ç½®
```python
# è®­ç»ƒå‚æ•°
config = {
    'batch_size': 16,
    'learning_rate': 1e-4,
    'weight_decay': 1e-4,
    'temperature': 0.07,
    'num_epochs': 50,
    'warmup_epochs': 5
}
```

### å¯åŠ¨è®­ç»ƒ
```bash
python train_object_guided_clip.py \
    --config configs/train_config.yaml \
    --data_path data/your_dataset \
    --output_dir outputs/experiment_1
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰ç‰¹å¾èåˆ
```python
class CustomFusion(nn.Module):
    def __init__(self):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads)
        self.fusion_conv = nn.Conv2d(...)
    
    def forward(self, mask_feat, object_feat, clip_feat):
        # æ³¨æ„åŠ›æœºåˆ¶èåˆ
        fused = self.attention(mask_feat, object_feat, clip_feat)
        return self.fusion_conv(fused)
```

### å¤šå°ºåº¦å¯¹è±¡å¤„ç†
```python
# ä¸åŒå°ºåº¦çš„å¯¹è±¡å¤„ç†
def multi_scale_processing(image, point_prompts):
    scales = [0.5, 1.0, 2.0]
    features = []
    
    for scale in scales:
        scaled_image = F.interpolate(image, scale_factor=scale)
        mask = model.generate_object_mask(scaled_image, point_prompts)
        feat = extract_features(mask, scaled_image)
        features.append(feat)
    
    return combine_multi_scale_features(features)
```

## ğŸ“Š å®éªŒç»“æœ

### å¯¹æ¯”å®éªŒ
| æ–¹æ³• | Image-to-Text R@1 | Text-to-Image R@1 | å¹³å‡æ’å |
|------|-------------------|-------------------|----------|
| åŸå§‹ CLIP | 78.3% | 76.9% | 4.2 |
| å¯¹è±¡å¼•å¯¼ CLIP | 85.2% | 83.7% | 2.3 |
| æ”¹è¿›å¹…åº¦ | +6.9% | +6.8% | -1.9 |

### æ¶ˆèå®éªŒ
| ç»„ä»¶ | Image-to-Text R@1 | ç§»é™¤å½±å“ |
|------|-------------------|----------|
| å®Œæ•´æ¨¡å‹ | 85.2% | - |
| ç§»é™¤æ©ç åˆ†æ”¯ | 81.4% | -3.8% |
| ç§»é™¤å¯¹è±¡åˆ†æ”¯ | 82.1% | -3.1% |
| ç§»é™¤ CLIP åˆ†æ”¯ | 79.6% | -5.6% |

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦æƒ…ã€‚

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# è¿è¡Œæµ‹è¯•
python -m pytest tests/

# ä»£ç æ ¼å¼åŒ–
black .
isort .
```

## ğŸ™ è‡´è°¢

- [Segment Anything Model 2](https://github.com/facebookresearch/segment-anything-2) - æä¾›å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›
- [CLIP](https://github.com/openai/CLIP) - å¼€åˆ›æ€§çš„è§†è§‰-è¯­è¨€æ¨¡å‹
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶

---